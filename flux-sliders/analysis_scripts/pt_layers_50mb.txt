lora_unet_transformer_blocks_0_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_0_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_1_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_2_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_3_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_4_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_5_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_6_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_7_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_8_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_9_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_10_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_11_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_12_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_13_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_14_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_15_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_16_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_17_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_k_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_k_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_k_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_v_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_v_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_v_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_q_proj.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_q_proj.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_add_q_proj.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_out_0.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_out_0.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_out_0.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_add_out.alpha torch.Size([]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_add_out.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_transformer_blocks_18_attn_to_add_out.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_0_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_1_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_2_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_3_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_4_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_5_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_6_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_7_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_8_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_9_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_10_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_11_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_12_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_13_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_14_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_15_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_16_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_17_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_18_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_19_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_20_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_21_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_22_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_23_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_24_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_25_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_26_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_27_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_28_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_29_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_30_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_31_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_32_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_33_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_34_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_35_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_36_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_q.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_q.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_q.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_k.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_k.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_k.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_v.alpha torch.Size([]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_v.lora_down.weight torch.Size([16, 3072]) torch.bfloat16
lora_unet_single_transformer_blocks_37_attn_to_v.lora_up.weight torch.Size([3072, 16]) torch.bfloat16
